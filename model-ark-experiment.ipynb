{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1317101,"sourceType":"datasetVersion","datasetId":642388}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/rishabhsingh18/model-ark-experiment?scriptVersionId=191330502\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom PIL import Image\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-08-05T21:01:39.084697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data preparation\ndef load_preprocess_data(data_path):\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    dataset = ImageFolder(root=data_path, transform=transform)\n    data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n    return data_loader\n# insert path to the datasets in ''\ndata_loader = load_preprocess_data('/kaggle/input/nih-chest-x-ray-14-224x224-resized')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Implementation","metadata":{}},{"cell_type":"code","source":"class StudentTeacherModel(nn.Module):\n    def __init__(self, base_model):\n        super(StudentTeacherModel, self).__init__()\n        self.base_model = base_model\n        # assuming 14 classes\n        self.classification_head = nn.Linear(base_model.fc.in_features, 14)\n        self.segmentation_head = nn.Conv2d(base_model.fc.in_features, 1, kernel_size=1)\n        \n    def forward(self, x):\n        features = self.base_model(x)\n        classification_output = self.classification_head(features)\n        segmentation_output = self.segmentation_head(features.unsqueeze(2).unsqueeze(3))\n        return classfication_output, segmentation_output\n\nbase_model = models.resnet50(pretrained=True)\nmodel = StudentTeacherModel(base_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training Loop\nfor epoch in range(10):\n    model.train()\n    running_loss = 0.0\n    for i, data in enumerate(data_loader, 0):\n        inputs, labels = data\n        optimizer.zero_grad()\n        outputs, _ = model(inputs)\n        loss = criterion (outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    print(f'Epoch {epoch+1}, Loss: {running_loss/len(data_loader)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"def evaluate_model(model, data_loader):\n    model.eval()\n    all_labels = []\n    all_predictions = []\n    with torch.no_grad():\n        for data in data_loader:\n            inputs, labels = data\n            outputs, _ = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            all_labels.extend(labels.numpy())\n            all_predictions.extend(predicted.numpy())\n    accuracy = accuracy_score(all_labels, all_predictions)\n    auc_roc = roc_auc_score(all_labels, all_predictions, multi_class='ovo')\n    print(f'Accuracy: {accuracy}, AUC-ROC: {auc_roc}')\n    \nevaluate_model(model, data_loader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analysis and Reporting via Matplotlib","metadata":{}},{"cell_type":"code","source":"# Plotting training loss\nloss_values = [running_loss/len(data_loader)]  # Accumulate this in the training loop\nplt.plot(range(10), loss_values)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss Curve')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model weights\ntorch.save(model.state_dict(), 'model_weights.pth')\n\n# Save the results to a CSV or a text file\nwith open('experiment_results.txt', 'w') as f:\n    f.write(f'Accuracy: {accuracy}\\n')\n    f.write(f'AUC-ROC: {auc_roc}\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References\n\nSure, here are some references that will be helpful for your experiment on creating a foundation model for chest X-ray analysis, inspired by the \"Foundation Ark\" paper:\n\n### Papers and Articles\n\n1. **Foundation Ark Paper**\n   - Ma, D., Lu, Z., Xiao, S., Niu, Y., and Li, J. (2023). *Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust Performance*. International Conference on Medical Imaging with Deep Learning (MIDL).\n   - [Link to Paper](https://doi.org/10.1007/978-3-031-43907-0_62)\n\n2. **CheXpert Dataset**\n   - Irvin, J., Rajpurkar, P., Ko, M., et al. (2019). *CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison*. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01), 590-597.\n   - [Link to Paper](https://arxiv.org/abs/1901.07031)\n\n3. **MIMIC-CXR Dataset**\n   - Johnson, A. E., Pollard, T. J., Greenbaum, N. R., et al. (2019). *MIMIC-CXR: A large publicly available database of labeled chest radiographs*. arXiv preprint arXiv:1901.07042.\n   - [Link to Paper](https://arxiv.org/abs/1901.07042)\n\n4. **ChestX-ray14 Dataset**\n   - Wang, X., Peng, Y., Lu, L., et al. (2017). *ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases*. IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n   - [Link to Paper](https://arxiv.org/abs/1705.02315)\n\n5. **PadChest Dataset**\n   - Bustos, A., Pertusa, A., Salinas, J. M., and de la Iglesia-Vay√°, M. (2020). *PadChest: A large chest x-ray image dataset with multi-label annotated reports*. Medical Image Analysis, 66, 101797.\n   - [Link to Paper](https://doi.org/10.1016/j.media.2020.101797)\n\n6. **Open-I Indiana University Dataset**\n   - Demner-Fushman, D., Antani, S., and Thoma, G. R. (2012). *MedPix: Web-based cross-referencing of radiology teaching file images and national library of medicine literature*. Journal of the American Medical Informatics Association, 19(3), 460-464.\n   - [Link to Paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3376183/)\n\n### Pretraining and Transfer Learning\n7. **Pretraining and Fine-Tuning Strategies**\n   - Kornblith, S., Shlens, J., and Le, Q. V. (2019). *Do Better ImageNet Models Transfer Better?*. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\n   - [Link to Paper](https://arxiv.org/abs/1805.08974)\n\n8. **Transfer Learning in Medical Imaging**\n   - Raghu, M., Zhang, C., Kleinberg, J., and Bengio, S. (2019). *Transfusion: Understanding Transfer Learning with Applications to Medical Imaging*. arXiv preprint arXiv:1902.07208.\n   - [Link to Paper](https://arxiv.org/abs/1902.07208)\n\n### Robustness and Fairness\n9. **Evaluating Model Robustness**\n   - Hendrycks, D., and Dietterich, T. (2019). *Benchmarking Neural Network Robustness to Common Corruptions and Perturbations*. arXiv preprint arXiv:1903.12261.\n   - [Link to Paper](https://arxiv.org/abs/1903.12261)\n\n10. **Fairness in Machine Learning**\n    - Barocas, S., Hardt, M., and Narayanan, A. (2019). *Fairness and Machine Learning*. fairmlbook.org.\n    - [Link to Book](https://fairmlbook.org/)\n\n### Tools and Frameworks\n11. **PyTorch Official Documentation**\n    - *PyTorch: Tensors and Dynamic neural networks in Python with strong GPU acceleration*. PyTorch.org.\n    - [Link to Documentation](https://pytorch.org/docs/stable/index.html)\n\n12. **Scikit-Learn Official Documentation**\n    - *Scikit-Learn: Machine Learning in Python*. scikit-learn.org.\n    - [Link to Documentation](https://scikit-learn.org/stable/documentation.html)\n","metadata":{}}]}